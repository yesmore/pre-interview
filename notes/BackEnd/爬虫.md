# 爬虫（Python版）

## 一、爬虫基础简介

### **前戏**

​    1.你是否在夜深人静的时候，想看一些会让你更睡不着的图片却苦于没有资源...
​    2.你是否在节假日出行高峰的时候，想快速抢购火车票成功...
​    3.你是否在网上购物的时候，想快速且精准的定位到口碑质量最好的商品...

**什么是爬虫**：

    - 通过编写程序，模拟浏览器上网，然后让其去互联网上抓取数据的过程。

**爬虫的价值**：
    - 实际应用
        - 就业

**爬虫究竟是合法还是违法的**？

- 在法律中是不被禁止
- 具有违法风险
- 善意爬虫  恶意爬虫

**爬虫带来的风险可以体现在如下**2方面：
    - 爬虫干扰了被访问网站的正常运营
        - 爬虫抓取了收到法律保护的特定类型的数据或信息

**如何在使用编写爬虫的过程中避免进入局子的厄运呢**？
    - 时常的优化自己的程序，避免干扰被访问网站的正常运行
        - 在使用，传播爬取到的数据时，审查抓取到的内容，如果发现了涉及到用户隐私
        商业机密等敏感内容需要及时停止爬取或传播

**爬虫在使用场景中的分类**

- 通用爬虫：
  抓取系统重要组成部分。抓取的是一整张页面数据。

-  聚焦爬虫：

  是建立在通用爬虫的基础之上。抓取的是页面中特定的局部内容。

- 增量式爬虫：
  检测网站中数据更新的情况。只会抓取网站中最新更新出来的数据。

**爬虫的矛与盾**

**反爬机制**
    门户网站，可以通过制定相应的策略或者技术手段，防止爬虫程序进行网站数据的爬取。

**反反爬策略**
    爬虫程序可以通过制定相关的策略或者技术手段，破解门户网站中具备的反爬机制，从而可以获取门户网站中相关的数据。

**robots.txt**协议：
    君子协议。规定了网站中哪些数据可以被爬虫爬取哪些数据不可以被爬取。



### **http**协议

   - 概念：就是`服务器`和`客户端`进行数据交互的一种形式。
   - **常用请求头信息**
     - User-Agent：请求载体的身份标识
     - Connection：请求完毕后，是断开连接还是保持连接

- **常用响应头信息**
     - Content-Type：服务器响应回客户端的数据类型

**https**协议：

    - 安全的超文本传输协议

**加密方式**

   - 对称秘钥加密
           - 可能被第三方拦截
   - 非对称秘钥加密
           - 速度慢
           - 公钥可能被篡改
   - 证书秘钥加密
           - 给公钥数字签名





## 二、requests模块

- 网络请求模块
  - urllib模块
  - **requests模块**

> requests模块：python中原生的一款基于网络请求的模块，功能非常强大，简单便捷，效率极高。
>
> 作用：**模拟浏览器发请求**。

#### 使用步骤

> requests模块的编码流程

- 指定url
  - UA伪装
  - 请求参数的处理
- 发起请求
   - 获取响应数据
- 持久化存储

#### 环境安装

 `pip install requests`

#### 实战编码

- 需求：爬取搜狗首页的页面数据

```python
#!/usr/bin/env python 
# -*- coding:utf-8 -*-
# - 需求：爬取搜狗首页的页面数据
import requests

if __name__ == "__main__":
    # step_1:指定url
    url = 'https://www.sogou.com/'
    # step_2:发起请求
    # get方法会返回一个响应对象
    response = requests.get(url=url)
    # step_3:获取响应数据.text返回的是字符串形式的响应数据
    page_text = response.text
    print(page_text)
    # step_4:持久化存储
    with open('./sogou.html', 'w', encoding='utf-8') as fp:
        fp.write(page_text)
    print('爬取数据结束！！！')
```



#### 实战巩固

   - 需求：爬取搜狗指定词条对应的搜索结果页面（简易网页采集器）
        - **UA检测**

            > UA：User-Agent（请求载体的身份标识）.
            >
            > 门户网站的服务器会检测对应请求的载体身份标识，如果检测到请求的载体身份标识为某一款浏览器，说明该请求是一个正常的请求。但是，如果检测到请求的载体身份标识不是基于某一款浏览器的，则表示该请求为不正常的请求（爬虫），则服务器端就很有可能拒绝该次请求。

        - **UA伪装**: 将对应的User-Agent封装到一个字典中

            > 让爬虫对应的请求载体身份标识伪装成某一款浏览器

            ```python
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'
            }
            ```

- 需求：破解百度翻译(获取翻译结果)

     - post请求（携带了参数）
     - 响应数据是一组json数据

- 需求：爬取豆瓣电影分类排行榜 https://movie.douban.com/中的电影详情数据

- 作业：爬取肯德基餐厅查询http://www.kfc.com.cn/kfccda/index.aspx中指定地点的餐厅数据

- 需求：爬取国家药品监督管理总局中基于中华人民共和国化妆品生产许可证相关数据
            http://scxk.nmpa.gov.cn:81/xk/
        
    - 动态加载数据
- 首页中对应的企业信息数据是通过ajax动态请求到的。
  
      http://125.35.6.84:81/xk/itownet/portal/dzpz.jsp?id=e6c1aa332b274282b04659a6ea30430a
      http://125.35.6.84:81/xk/itownet/portal/dzpz.jsp?id=f63f61fe04684c46a016a45eac8754fe
  
    - 通过对详情页url的观察发现：
        - url的域名都是一样的，只有携带的参数（id）不一样
        - id值可以从首页对应的ajax请求到的json串中获取
        - 域名和id值拼接处一个完整的企业对应的详情页的url
    - 详情页的企业详情数据也是动态加载出来的
        - http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById
        - http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById
        - 观察后发现：
            - 所有的post请求的url都是一样的，只有参数id值是不同。
            - 如果我们可以批量获取多家企业的id后，就可以将id和url形成一个完整的详情页对应详情数据的ajax请求的url

## 三、数据解析

> 聚焦爬虫、正则、bs4、xpath

#### 聚焦爬虫

定义：爬取页面中指定的页面内容。

编码流程：
- 指定url
- 发起请求
- 获取响应数据
- 数据解析
- 持久化存储



#### 数据解析分类

- 正则
- bs4
- xpath（***）

**数据解析原理概述**

   - 解析的局部的文本内容都会在**标签之间或者标签对应的属性中进行存储**
- 1.**进行指定标签的定位**
- 2.标签或者标签对应的**属性**中存储的**数据**值进行**提取（解析）**



##### **正则解析**

回顾

![正则表达式](C:\Users\86136\Pictures\学习\rep.png)

```python
import re
#提取出python
key="javapythonc++php"
re.findall('python',key)[0]
########################################################
#提取出hello world
key="
hello world
"
re.findall('
(.*)
',key)[0]
########################################################
#提取170
string = '我喜欢身高为170的女孩'
re.findall('\d+',string)
########################################################
#提取出http://和https://
key='http://www.baidu.com and https://boob.com'
re.findall('https?://',key)
########################################################
#提取出hello
key='lalalahellohahah' #输出hello
re.findall('<[Hh][Tt][mM][lL]>(.*)',key)
########################################################
#提取出hit. 
key='bobo@hit.edu.com'#想要匹配到hit.
re.findall('h.*?\.',key)
########################################################
#匹配sas和saas
key='saas and sas and saaas'
re.findall('sa{1,2}s',key)
```



##### **bs4进行数据解析**

    - 数据解析的原理：
        - 1.标签定位
        - 2.提取标签、标签属性中存储的数据值
- bs4数据解析的原理：
     - 1.实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中
     - 2.通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取
- 环境安装：
     - `pip install bs4`
     - `pip install lxml`
- 如何实例化BeautifulSoup对象：
     - `from bs4 import BeautifulSoup`
     - 对象的实例化：
         - 1.将本地的html文档中的数据加载到该对象中
                 `fp = open('./test.html','r',encoding='utf-8')`
                ` soup = BeautifulSoup(fp,'lxml')`
         - 2.将互联网上获取的页面源码加载到该对象中
                 `page_text = response.text`
             ` soup = BeatifulSoup(page_text,'lxml')`
     - 提供的用于数据解析的方法和属性：
         - `soup.tagName`:返回的是文档中第一次出现的tagName对应的标签
         - `soup.find()`:
             - `find('tagName')`:等同于`soup.div`
             - 属性定位：
                 `soup.find('div',class_/id/attr='song')`
         - `soup.find_all('tagName')`:返回符合要求的所有标签（列表）
     - `select`：
         - select('某种选择器（id，class，标签...选择器）'),返回的是一个列表。
         - 层级选择器：
             - `soup.select('.tang > ul > li > a')`：>表示的是一个层级
             - `oup.select('.tang > ul a')`：空格表示的多个层级
     - 获取标签之间的文本数据：
         - soup.a.text/string/get_text()
         - text/get_text():可以获取某一个标签中所有的文本内容
         - string：只可以获取该标签下面直系的文本内容
     - 获取标签中属性值：
         - soup.a['href']



##### xpath解析

> 最常用且最便捷高效的一种解析方式。通用性。

- xpath解析原理：
    - 1.实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中。
    - 2.调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获。
- 环境的安装：
    - pip install lxml
- 如何实例化一个etree对象:from lxml import etree
    - 1.将**本地**的html文档中的源码数据加载到etree对象中：
        etree.parse(filePath)
    - 2.可以将从**互联网**上获取的源码数据加载到该对象中
        etree.HTML('page_text')
    - xpath('xpath表达式')
- xpath表达式:
    - **/**: 表示的是从根节点开始定位。表示的是一个层级。
    - **//**: 表示的是多个层级。可以表示从任意位置开始定位。
    - **属性定位**：//div[@class='song'] tag[@attrName="attrValue"]
    - **索引定位**：//div[@class="song"]/p[3] 索引是从1开始的。
    - **取文本**：
        - /text() 获取的是标签中直系的文本内容
        - //text() 标签中非直系的文本内容（所有的文本内容）
    - **取属性**：
        /@attrName     ==>img/src

作业：
    爬取站长素材中免费简历模板





## 四、验证码识别

##### 验证码和爬虫之间的关系

- 反爬机制：验证码.识别验证码图片中的数据，用于模拟登陆操作。

##### 识别验证码的操作

    - 人工肉眼识别。（不推荐）
        - 第三方自动识别（推荐）
        - 云打码：http://www.yundama.com/demo.html
- 云打码的使用流程：
  - 注册：普通和开发者用户
  - 登录：
      - 普通用户的登录：查询该用户是否还有剩余的题分
      - 开发者用户的登录：
          - 创建一个软件：我的软件-》添加新软件-》录入软件名称-》提交（软件id和秘钥）
          - 下载示例代码：开发文档-》点此下载：云打码接口DLL-》PythonHTTP示例下载

##### 实战：识别古诗文网登录页面中的验证码

使用打码平台识别验证码的编码流程：

- 将验证码图片进行本地下载
- 调用平台提供的示例代码进行图片数据识别





## 五、request高级

#### 模拟登录

- 爬取基于某些用户的用户信息。

#### 需求1

> 对人人网进行模拟登录。

   - 点击登录按钮之后会发起一个post请求
- post请求中会携带登录之前录入的相关的登录信息（用户名，密码，验证码......）
- 验证码：每次请求都会变化



#### 需求2

> 爬取当前用户的相关的用户信息（个人主页中显示的用户信息）



#### cookie与session

http/https协议特性：无状态。
没有请求到对应页面数据的原因：
    发起的第二次基于个人主页页面请求的时候，服务器端并不知道该此请求是基于登录状态下的请求。
cookie：用来让服务器端记录客户端的相关状态。

   - 手动处理：通过抓包工具获取cookie值，将该值封装到headers中。（不建议）
- 自动处理：
     - cookie值的来源是哪里？
         - 模拟登录post请求后，由服务器端创建。
     - **session会话对象**：
       - 作用：
           1.可以进行请求的发送。
           2.如果请求过程中产生了cookie，则该cookie会被自动存储/携带在该session对象中。
     - 创建一个session对象：session = requests.Session()
     - 使用session对象进行模拟登录post请求的发送（cookie就会被存储在session中）
     - session对象对个人主页对应的get请求进行发送（携带了cookie）

     

#### 代理

> 破解封IP这种反爬机制。

什么是代理：

   - 代理服务器。

代理的作用：

   - 突破自身IP访问的限制。
- 隐藏自身真实IP

代理相关的网站：

  - 快代理
  - 西祠代理
  - www.goubanjia.com

代理ip的类型：

  - http：应用到http协议对应的url中
  - https：应用到https协议对应的url中

代理ip的匿名度：
   - 透明：服务器知道该次请求使用了代理，也知道请求对应的真实ip
- 匿名：知道使用了代理，不知道真实ip
- 高匿：不知道使用了代理，更不知道真实的ip



## 六、异步爬虫

目的：在爬虫中使用异步实现高性能的数据爬取操作。

异步爬虫的方式：

- 1.多线程，多进程（不建议）：
  好处：可以为相关阻塞的操作单独开启线程或者进程，阻塞操作就可以异步执行。
  弊端：无法无限制的开启多线程或者多进程。
- 2.线程池、进程池（适当的使用）：
  好处：我们可以降低系统对进程或者线程创建和销毁的一个频率，从而很好的降低系统的开销。
  弊端：池中线程或进程的数量是有上限。

- 3.单线程+异步协程（推荐）：
    **event_loop**：事件循环，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上，
    当满足某些条件的时候，函数就会被循环执行。

    **coroutine**：协程对象，我们可以将协程对象注册到事件循环中，它会被事件循环调用。
    我们可以使用 async 关键字来定义一个方法，这个方法在调用时不会立即被执行，而是返回
    一个协程对象。

    **task**：任务，它是对协程对象的进一步封装，包含了任务的各个状态。

    **future**：代表将来执行或还没有执行的任务，实际上和 task 没有本质区别。

    **async** 定义一个协程.

    await 用来挂起阻塞方法的执行。

